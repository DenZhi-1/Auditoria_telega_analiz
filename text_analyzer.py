import logging
import re
import nltk
from typing import Dict, List, Any, Tuple
from collections import Counter
import asyncio

logger = logging.getLogger(__name__)

try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    pass

from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import string

class TextAnalyzer:
    """AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.russian_stopwords = set(stopwords.words('russian'))
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self.positive_words = {
            '—Ö–æ—Ä–æ—à–∏–π', '–æ—Ç–ª–∏—á–Ω—ã–π', '–ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã–π', '–ª—É—á—à–∏–π',
            '—É–¥–æ–±–Ω—ã–π', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–ø—Ä–æ—Å—Ç–æ–π', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π', '–ø–æ–ª–µ–∑–Ω—ã–π',
            '–≤–∞–∂–Ω—ã–π', '–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π', '—É—Å–ø–µ—à–Ω—ã–π', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π', '–ø–æ–ø—É–ª—è—Ä–Ω—ã–π',
            '–ª—é–±–∏–º—ã–π', '–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π', '—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π',
            '–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π', '–∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π', '—è—Ä–∫–∏–π', '–∫—Ä–∞—Å–∏–≤—ã–π', '—Å—Ç–∏–ª—å–Ω—ã–π'
        }
        
        self.negative_words = {
            '–ø–ª–æ—Ö–æ–π', '—É–∂–∞—Å–Ω—ã–π', '—Å–∫—É—á–Ω—ã–π', '—Å–ª–æ–∂–Ω—ã–π', '—Ç—Ä—É–¥–Ω—ã–π',
            '–¥–æ—Ä–æ–≥–æ–π', '–¥–µ—à–µ–≤—ã–π', '—Å—Ç–∞—Ä—ã–π', '–º–µ–¥–ª–µ–Ω–Ω—ã–π', '–ø—Ä–æ–±–ª–µ–º–Ω—ã–π',
            '—Å–ª–∞–±—ã–π', '–æ–ø–∞—Å–Ω—ã–π', '—Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–π', '–Ω–µ—É–¥–æ–±–Ω—ã–π', '–Ω–µ–ø–æ–Ω—è—Ç–Ω—ã–π',
            '–≥–ª—É–ø—ã–π', '–±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π', '–Ω–µ–Ω—É–∂–Ω—ã–π', '—É—Å—Ç–∞—Ä–µ–≤—à–∏–π', '—Å–ª–æ–º–∞–Ω–Ω—ã–π',
            '–æ—à–∏–±–∫–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫', '–º–∏–Ω—É—Å', '–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π'
        }
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        self.text_categories = {
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π': ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–∫–æ–¥', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö'],
            '–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π': ['–æ–±—É—á–µ–Ω–∏–µ', '–∫—É—Ä—Å', '–ª–µ–∫—Ü–∏—è', '—É—Ä–æ–∫', '–∑–Ω–∞–Ω–∏–µ', '–Ω–∞—É–∫–∞'],
            '–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π': ['–ø—Ä–æ–¥–∞–∂–∞', '–ø–æ–∫—É–ø–∫–∞', '—Ü–µ–Ω–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–º–∞–≥–∞–∑–∏–Ω'],
            '—Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π': ['—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–µ', '–∏–≥—Ä–∞', '—é–º–æ—Ä', '–ø—Ä–∏–∫–æ–ª', '–º–µ–º', '—Å–º–µ—à–Ω–æ'],
            '–Ω–æ–≤–æ—Å—Ç–Ω–æ–π': ['–Ω–æ–≤–æ—Å—Ç—å', '—Å–æ–±—ã—Ç–∏–µ', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–∞–Ω–æ–Ω—Å'],
            '—Å–æ—Ü–∏–∞–ª—å–Ω—ã–π': ['—Å–æ–æ–±—â–µ—Å—Ç–≤–æ', '–≥—Ä—É–ø–ø–∞', '–¥—Ä—É–∑—å—è', '–æ–±—â–µ–Ω–∏–µ', '–¥–∏—Å–∫—É—Å—Å–∏—è'],
            '–ª–∏—á–Ω—ã–π': ['–æ–ø—ã—Ç', '–∏—Å—Ç–æ—Ä–∏—è', '—Ä–∞—Å—Å–∫–∞–∑', '–º–Ω–µ–Ω–∏–µ', '—Å–æ–≤–µ—Ç', '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è']
        }
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏
        self.emotion_words = {
            '—Ä–∞–¥–æ—Å—Ç—å': {'—Ä–∞–¥', '—Å—á–∞—Å—Ç–ª–∏–≤', '–¥–æ–≤–æ–ª–µ–Ω', '–≤–æ—Å—Ç–æ—Ä–≥', '—É—Ä–∞', '—É—Å–ø–µ—Ö'},
            '–≥—Ä—É—Å—Ç—å': {'–≥—Ä—É—Å—Ç–Ω–æ', '–ø–µ—á–∞–ª—å', '—Ç–æ—Å–∫–∞', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ', '–∂–∞–ª—å'},
            '–≥–Ω–µ–≤': {'–∑–ª–æ–π', '—Å–µ—Ä–¥–∏—Ç', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω', '–≤–æ–∑–º—É—â–µ–Ω', '–±–µ—Å–∏—Ç'},
            '—Å—Ç—Ä–∞—Ö': {'–±–æ—é—Å—å', '—Å—Ç—Ä–∞—à–Ω–æ', '–æ–ø–∞—Å–Ω–æ', '—Ç—Ä–µ–≤–æ–≥–∞', '–ø–µ—Ä–µ–∂–∏–≤–∞—é'},
            '—É–¥–∏–≤–ª–µ–Ω–∏–µ': {'—É–¥–∏–≤–ª–µ–Ω', '–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–ª—é–±–æ–ø—ã—Ç–Ω–æ', '–≤–∞—É'},
            '–¥–æ–≤–µ—Ä–∏–µ': {'–¥–æ–≤–µ—Ä—è—é', '–Ω–∞–¥–µ–∂–Ω—ã–π', '–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π', '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–±–µ–∑–æ–ø–∞—Å–Ω–æ'}
        }
    
    def preprocess_text(self, text: str) -> List[str]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text_lower = text.lower()
        
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã
        text_clean = re.sub(r'[^\w\s]', ' ', text_lower)
        text_clean = re.sub(r'\d+', ' ', text_clean)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = word_tokenize(text_clean, language='russian')
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        filtered_tokens = [
            token for token in tokens 
            if token not in self.russian_stopwords 
            and len(token) > 2
            and token not in string.punctuation
        ]
        
        return filtered_tokens
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        tokens = self.preprocess_text(text)
        
        if not tokens:
            return {'score': 0, 'label': 'neutral', 'confidence': 0}
        
        positive_count = 0
        negative_count = 0
        
        for token in tokens:
            if token in self.positive_words:
                positive_count += 1
            elif token in self.negative_words:
                negative_count += 1
        
        total_words = len(tokens)
        
        if total_words == 0:
            return {'score': 0, 'label': 'neutral', 'confidence': 0}
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ü–µ–Ω–∫—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç -1 –¥–æ 1
        sentiment_score = (positive_count - negative_count) / total_words
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–∫—É
        if sentiment_score > 0.1:
            label = 'positive'
            confidence = min(1.0, sentiment_score)
        elif sentiment_score < -0.1:
            label = 'negative'
            confidence = min(1.0, -sentiment_score)
        else:
            label = 'neutral'
            confidence = 1.0 - min(abs(sentiment_score), 1.0)
        
        return {
            'score': round(sentiment_score, 3),
            'label': label,
            'confidence': round(confidence, 3),
            'positive_words': positive_count,
            'negative_words': negative_count,
            'total_words': total_words
        }
    
    def extract_keywords(self, text: str, top_n: int = 20) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        tokens = self.preprocess_text(text)
        
        if not tokens:
            return []
        
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        word_freq = Counter(tokens)
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ, –Ω–æ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞
        common_words = {'—ç—Ç–æ—Ç', '—Ç–∞–∫–æ–π', '–∫–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '–æ—á–µ–Ω—å', '–º–æ–∂–Ω–æ'}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = []
        for word, count in word_freq.most_common(top_n * 2):
            if word not in common_words and count > 1:
                keywords.append({
                    'word': word,
                    'count': count,
                    'frequency': count / len(tokens)
                })
                
                if len(keywords) >= top_n:
                    break
        
        return keywords
    
    def categorize_text(self, text: str) -> List[Dict]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
        tokens = self.preprocess_text(text)
        tokens_set = set(tokens)
        
        categories = []
        
        for category, keywords in self.text_categories.items():
            matches = 0
            for keyword in keywords:
                if keyword in tokens_set:
                    matches += 1
            
            if matches > 0:
                score = matches / len(keywords)
                categories.append({
                    'name': category,
                    'score': round(score, 3),
                    'matches': matches
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        categories.sort(key=lambda x: x['score'], reverse=True)
        
        return categories[:5]
    
    def analyze_emotions(self, text: str) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É —Ç–µ–∫—Å—Ç–∞"""
        tokens = self.preprocess_text(text)
        tokens_set = set(tokens)
        
        emotions = {}
        
        for emotion, words in self.emotion_words.items():
            matches = len([word for word in words if word in tokens_set])
            
            if matches > 0:
                score = matches / len(words)
                emotions[emotion] = round(score, 3)
        
        return emotions
    
    def calculate_readability(self, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ (0-100)"""
        if not text:
            return 0.0
        
        sentences = sent_tokenize(text, language='russian')
        words = self.preprocess_text(text)
        
        if not sentences or not words:
            return 0.0
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ö
        avg_sentence_length = len(words) / len(sentences)
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        avg_word_length = sum(len(word) for word in words) / len(words)
        
        # –û—Ü–µ–Ω–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–∞—è —Ñ–æ—Ä–º—É–ª–∞)
        # –ß–µ–º –∫–æ—Ä–æ—á–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Å–ª–æ–≤–∞, —Ç–µ–º –≤—ã—à–µ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å
        readability = 100 - (avg_sentence_length * 2 + avg_word_length * 5)
        
        return max(0, min(100, readability))
    
    def generate_recommendations(self, analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        recommendations = []
        
        sentiment = analysis.get('sentiment', {})
        readability = analysis.get('readability_score', 0)
        keywords = analysis.get('keywords', [])
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if sentiment.get('label') == 'negative':
            if sentiment.get('score', 0) < -0.3:
                recommendations.append("üìâ –¢–µ–∫—Å—Ç –∏–º–µ–µ—Ç —Å–∏–ª—å–Ω—É—é –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –æ–∫—Ä–∞—Å–∫—É. "
                                      "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫.")
            else:
                recommendations.append("‚ö†Ô∏è –¢–µ–∫—Å—Ç –∏–º–µ–µ—Ç –ª–µ–≥–∫—É—é –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é –æ–∫—Ä–∞—Å–∫—É. "
                                      "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏.")
        
        elif sentiment.get('label') == 'positive':
            if sentiment.get('score', 0) > 0.3:
                recommendations.append("üìà –û—Ç–ª–∏—á–Ω–∞—è –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å! "
                                      "–¢–∞–∫–∏–µ —Ç–µ–∫—Å—Ç—ã —Ö–æ—Ä–æ—à–æ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –∞—É–¥–∏—Ç–æ—Ä–∏–µ–π.")
            else:
                recommendations.append("üëç –¢–µ–∫—Å—Ç –∏–º–µ–µ—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—É—é –æ–∫—Ä–∞—Å–∫—É. "
                                      "–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤.")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        if readability < 40:
            recommendations.append("üîç –ù–∏–∑–∫–∞—è —á–∏—Ç–∞–µ–º–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞. "
                                  "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞–∑–±–∏—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–µ.")
        elif readability < 60:
            recommendations.append("üìñ –°—Ä–µ–¥–Ω—è—è —á–∏—Ç–∞–µ–º–æ—Å—Ç—å. "
                                  "–ú–æ–∂–Ω–æ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è.")
        else:
            recommendations.append("‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —á–∏—Ç–∞–µ–º–æ—Å—Ç—å! –¢–µ–∫—Å—Ç –ª–µ–≥–∫–æ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è.")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if keywords:
            top_keywords = [k['word'] for k in keywords[:5]]
            recommendations.append(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö: {', '.join(top_keywords)}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        text_length = analysis.get('text_length', 0)
        if text_length < 500:
            recommendations.append("üìù –¢–µ–∫—Å—Ç –¥–æ–≤–æ–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π. "
                                  "–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π –∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è —Ç–µ–º—ã.")
        elif text_length > 3000:
            recommendations.append("üìö –¢–µ–∫—Å—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π. "
                                  "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–±–∏—Ç—å –µ–≥–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π.")
        
        return recommendations[:5]
    
    async def analyze_text(self, text: str) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return {'error': '–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π'}
        
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ (–¥–ª–∏–Ω–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [
            asyncio.to_thread(self.analyze_sentiment, text),
            asyncio.to_thread(self.extract_keywords, text, 15),
            asyncio.to_thread(self.categorize_text, text),
            asyncio.to_thread(self.analyze_emotions, text),
            asyncio.to_thread(self.calculate_readability, text)
        ]
        
        results = await asyncio.gather(*tasks)
        
        analysis = {
            'text_length': len(text),
            'unique_words': len(set(self.preprocess_text(text))),
            'avg_sentence_length': len(self.preprocess_text(text)) / 
                                  max(1, len(sent_tokenize(text, language='russian'))),
            'sentiment': results[0],
            'keywords': results[1],
            'topics': results[2],
            'emotions': results[3],
            'readability_score': round(results[4], 1)
        }
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        analysis['recommendations'] = self.generate_recommendations(analysis)
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω. –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {analysis['sentiment']['label']}")
        
        return analysis
    
    def generate_text_report(self, analysis: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É"""
        report_lines = [
            "üß† –û–¢–ß–ï–¢ –ü–û AI-–ê–ù–ê–õ–ò–ó–£ –¢–ï–ö–°–¢–ê",
            "=" * 50,
            f"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {analysis.get('text_length', 0):,} —Å–∏–º–≤–æ–ª–æ–≤",
            f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {analysis.get('unique_words', 0)}",
            f"–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å: {analysis.get('readability_score', 0)}/100",
            ""
        ]
        
        # –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        sentiment = analysis.get('sentiment', {})
        if sentiment:
            sentiment_label = {
                'positive': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è',
                'negative': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è', 
                'neutral': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è'
            }.get(sentiment.get('label', 'neutral'), '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è')
            
            report_lines.append(f"–¢–û–ù–ê–õ–¨–ù–û–°–¢–¨: {sentiment_label}")
            report_lines.append(f"–û—Ü–µ–Ω–∫–∞: {sentiment.get('score', 0):.3f}")
            report_lines.append(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {sentiment.get('confidence', 0):.1%}")
            report_lines.append("")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = analysis.get('keywords', [])
        if keywords:
            report_lines.append("–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê (—Ç–æ–ø-10):")
            for i, kw in enumerate(keywords[:10], 1):
                report_lines.append(f"{i}. {kw['word']} ({kw['count']} —Ä–∞–∑)")
            report_lines.append("")
        
        # –¢–µ–º—ã
        topics = analysis.get('topics', [])
        if topics:
            report_lines.append("–û–°–ù–û–í–ù–´–ï –¢–ï–ú–´:")
            for topic in topics[:5]:
                report_lines.append(f"‚Ä¢ {topic['name']}: {topic['score']:.1%}")
            report_lines.append("")
        
        # –≠–º–æ—Ü–∏–∏
        emotions = analysis.get('emotions', {})
        if emotions:
            report_lines.append("–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –û–ö–†–ê–°–ö–ê:")
            for emotion, score in emotions.items():
                if score > 0.1:
                    report_lines.append(f"‚Ä¢ {emotion}: {score:.1%}")
            report_lines.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            report_lines.append("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            for i, rec in enumerate(recommendations, 1):
                report_lines.append(f"{i}. {rec}")
        
        return "\n".join(report_lines)
